- in addPrefNode, L\b takes significant time
- parallel? all grad descents can be done independently
- ADMM stopping ocndition
- how to evaluate precision recall for multiple time steps. use predicted attachments each time or real?
- make step size argument instead of hard coded 
- use old a[i] to initialize gradient descent?
- compare different edges added
- sparse vs lufact solve?




Graphs
tree
cycle
preferential attachment
grid*
small world
Erdos renyi


B sparsity
sparse
dense

B norm
tune couple of methods
compensate with a_0



look at expected # of attachments
histogram of probs



include("optimization.jl")
using optimization
using JLD

data = load("graph_jld");
L = data["laplacians"];
A = data["connections"];
a_0 = data["a_0"];
genb = data["b"];


rho = 1;
lambda = 0.00005;




pred_b = optimization.ADMM_grad_para(A,L,rho,lambda,a_0);




